Monday 15/6/2020
- Setup and configured dev environment: VSCode on Windows connected to remote Debian WSL, configured C++ Intellisense to use g++ and boost libraries
- Figured out how to include boost and compile with g++
- Installed CMake and Native Debug VSCode plugins
- todo: test CMake compilation, test boost filesystem code, read C++ project structure webpages (see saved Firefox session 15/6)

Tues 16/6/2020
- Made decision to use std::filesystem from C++17 preferably over boost::filesystem - standard lib vs 3rd party include
- Read C++ project structure (directory layout conventions) webpages (see saved Firefox session 15/6)

Wed 17/6/2020
- Altered directory structure to follow std form (/include, /src, /build directories)
- Configured CMakeLists.txt and correct building of project with cmake, including correct linking of std::filesystem
- Built and tested inotify-cpp library to watch a directory

Thur 18/6/2020
- Read through code of fswatch library, providing multiplatform filesystem watches - uses inotify and stat() on Linux
- Determined inotify alone is insufficient - cannot determine file changes if service is not running e.g. machine off, program crash
    - Still need a local database/log of local files, possibly though stat() or more likely std::filesystem.
    - Polling of large directories and many files will be slow. Ideal solution is inotify when service is running, with semi-regular file index polling/updates for best efficiency.
- Basic getting of file attributes with std::filesystem
    - Determine which file attributes are required
- Determine database system - sqlite? slow lookups when reading from file? hashmaps in memory, but then need to serialise to save when service not running
    - SQLite    - unencrypted db file on host, encrypt as a regular backed up file and send to host for backup
                - can be faster than I/O, small size, ACID transactions, well tested
                - SQL query language
    - file      - requires serialisation, constant I/O or bucketing of transactions
                - fast for a relatively small number of items
                - possible to do this with Boost MultiIndex MRU object cache or Boost lib serialisation

Fri 19/6/2020
- SQLite file index database connection made and libraries linked from code
- Separate FileIndex class prototype for index functions/capability
- todo: File integrity checks with hashes - compute before upload, and after, benchmark various hashing algos e.g. md5, sha-1, sha-256
    - http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A91D1D99A03C47E5E4E73AB012A65D3B?doi=10.1.1.657.9552&rep=rep1&type=pdf
- desired feature/improvement: inotify running in addition to regular file system polling and indexdb updates

Mon 22/6/2020
- Created abstracted DB class to handle DB functions (e.g. open/close db, execute SQL code with error handling), new main class "enclone" to tie together all other classes
- enclone class includes vector of shared_pts to Watch* objects, allowing multiple watch directories to be added
    - todo: regularly save this in SQLite db for persistance on crash/restart, and restore from db on start
- DB structure - 2 tables created and initialised from code
    - WatchDirs - path
    - FileIndex - IDX/filename/path/mod time/size/file hash

Tue 23/6/2020
- Changed Watch class to have single instance with a vector of watched directories/files inside
- Added recursive params to recursively add directories to Watch if bool recursive is true
- Researching how to develop into a linux daemon/service (encloned), ability to add watch files/directories from cmd line - need interprocess communcation (IPC)
    - use new style systemd daemon rather than old SysV daemons - guarantees daemon starts with a clean context, avoids issues with lost file descriptors when a process is forked the old way
    - see https://manpages.debian.org/testing/systemd/daemon.7.en.html
    - IPC:
        - 1. run binary via systemd and have it load a config file or from db for file watches etc, wrapper program can write to config/db however this would mean constant reloading of config files needs to be done (SIGHUP signal to systemd tells it to reload config)
            - issue with file locks with SQLite - need to ensure locks are respected/retry transactions - daemon can only read db if it notices mod time has changed
        - 2. alternatively open a socket to local process to allow control via another client config application - also see above link and sd_listen_fds(3) and sd-daemon(3)
            - "unix domain", local socket required - possible via Boost.Asio
            - allows possibility to extend to inter-computer communcation i.e. remote management
            - preferred option
        - 3. named pipes can share information between processes
            - should be fairly simple, inbuilt to linux, only one way communcation
        - 4. Boost.Interprocess - allows shared memory/messages between processes, but unecessary, do not require shared objects etc, only simple messages
    - for development/testing/debugging, seems easier to run daemon in foreground, then amend code later to run in systemd e.g. have systemd create and activate sockets as recommended, rather than manually creating them
- Include Boost.Asio and created new Socket.h classes - code compiles and runs but is not async so blocks further code from running

Wed 24/6/2020
- Populated watchFiles with files to watch and their attributes and abstracted and tidied up elements of the Watch::addWatch() method
- Changed vectors for watchFiles and watchDir into unordered_map, to avoid duplicate paths being added
    - watchDirs key is path, value contains flag for if watched directory is recursively watched
    - watchFiles key is path, value contains the last mod time of the file
    - added check/error message for duplicate file/directory when attempting to add a watch
- Added main execLoop to scan for:
    1. changes in files (change in file mod time)
    2. file or directory deletion (file/directory does not exist)
    3. new file/directory is added (if the watched directory recursive flag is true, new directory and contained files added)
- todo: research std::thread and std::jthread - ideally need to run execLoop is separate thread, and Socket in a separate thread to avoid blocking

Thur 25/6/2020
- Added path check for file/directory existing before adding
- Ammended fileIndex to store last modified time as std::time_t (unix time as integer) to enable storage in DB
- Dump dirIndex and fileIndex to DB, unique specifier on path columns to avoid duplicates
- Added a "bucket" for SQL commands in Watch - stringstream sqlQueue will keep appending SQL queries to be executed at once in a bucket similar to Nagles Algorithm

Fri 26/6/2020
- Added DB updates when file/directory is removed or changed
- Changed sqlQueue to work more efficiently by not attempting to add entire fileIndex/dirIndex map contents each time. SQL insert/delete/update queries only queued on initial add/removal/update.

Sun 28/6/2020
- Researched C++ multithreading - std::thread and std::jthread
    - How to request a thread to stop with jthread: https://medium.com/@vgasparyan1995/a-new-thread-in-c-20-jthread-ebd121ae8906 (C++20)
        - jthread not supported in latest stable Debian g++ - full C++20 features not yet available, similarly cannot use std::osyncstream to sync output stream between threads. Only a few experimental features of c++20 are available via flag -std=c++2a.
    - std::mutex is required to lock/unlock shared data between threads, including cout to avoid garbled output. For processes that naturally come to an end, std::lock_guard is preferable - this automatically locks then unlocks when function is complete. mutex::lock() is required for our infinitely looping polling/DB threads.
    - As our threads will be infinite loops, running for the duration of the program, use an "std::atomic<bool> stopFlag" as the thread loop condition as this is threadsafe. This atomic bool can then be safely set from outside the threads when we want to close down the thread/loop.
    - test/thread-test.cpp created to test multi-threading and above concepts

Mon 29/6/2020
- Reading C++ Concurrency in Action, PRACTICAL MULTITHREADING by ANTHONY WILLIAMS
    - Considerations from p304:
        - Which data needs to be protected from concurrent access?
        - How do you ensure that the data is protected?
        - Where in the code could other threads be at this time?
        - Which mutexes does this thread hold?
        - Which mutexes might other threads hold?
        - Are there any ordering requirements between the operations done in this thread and those done in another? How are those requirements enforced?
        - Is the data loaded by this thread still valid? Could it have been modified by other threads?
        - If you assume that another thread could be modifying the data, what would that mean and how could you ensure that this never happens?
- Multi-threading:
    - Implemented the Watch::scanFileChange() and Watch::execQueuedSQL loop in a separate thread run from enclone class - see Watch::execThread() for loop
    - Socket now runs in a separate thread also, however significant work still required to test/check how socket works
    - Threads are terminated by setting std::atomic_bool flag in enclone class
    - Deleted copy constructors from Watch and Socket class - these classes should not be copied. 
        - Do not call a new thread on a class object as the class object is copied by default, and mutexes are unable to be copied. 
        - Call instead on a pointer to an entry function of an existing object i.e. std::thread t(&foo::bar, &f) and not std::thread t5(foo)
- todo: ensure correct mutexes and locks are in place - wait until further classes are written and have a better understanding of what resources will be shared
- todo: exception handling for file operations required?
- todo: does DB update want to be in the same thread as file system change? possibly not, in any case this wants to be more infrequent than filesystem polling - use global define/vars/config to set update times

Tue 30/6/2020
- Downloaded AWS C++ SDK and read through code, setup S3 bucket to test uploads
- Read AWS SDK documentation - https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/welcome.html
- Read AWS S3 documentation

Wed 1/7/2020
- AWS S3 remote
    - Setup AWS CLI with correct credentials from AWS Educate/Vocareum (Account Details -> AWS CLI to get .aws/credentials file)
    - Built relevant parts of AWS SDK from source (s3 and transfer manager)
        - see https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/setup.html
        - sudo cmake -D CMAKE_BUILD_TYPE=Release -D BUILD_ONLY="s3;transfer"
    - Included AWS SDK in enclone, tested basic API call/connection and correct compilation with CMAKE
    - Added calls to S3 to list available buckets and list all objects within that bucket
        - todo: does AWS SDK want to be included as static or shared lib? BUILD_SHARED_LIBS CMAKE var
        - todo: design generic remote interface/class structure/methods i.e. upload()
- Started to design a flat encrypted file structure for remote locations
    - Weakness of rclone is identical filenames encrypt to identical filenames on remote and directory structure is retained
    - Potential solution is filename of encrypted file on remote is hash of original unencrypted file. Index contains mapping between original filepath and hash
        - All encrypted files are uploaded in a flat directory structure, and structure is restored from index on restore from remote
        - File hashes are already required to ensure integrity after upload/download so hashing is already in place
        - Possible weakness is if you lose the index, you lose the directory structure and filenames - however index can also be encrypted and uploaded to remote. How expensive to keep encrypting/uploading index when modified? Should be small overhead.
            - Known structure of index could pose a weakness for some encryption algorithms e.g. some block ciphers not using a Feedback Mode - can be avoided with appropriate encryption algos

Thur 2/7/2020
- Designed generic Queue class for a FIFO queue of objects to upload/delete on remote storage, using std::deque (double ended queue) for efficient addition and removal on both ends
- Amended S3.h to include generic Queue object, S3.h to run in separate thread, regularly uploading and deleting objects from the queue on the remote
    - Tested uploadQueue to call upload function before actually uploading to S3

Fri 3/7/2020
- Added Remote.h stub class for later addition allowing handling of multiple remotes. 
    - Remotes do not need to run in their own separate thread, they can all run under the Remote thread sequentially
- Tested actual upload of watched files to S3 bucket using AWS C++ SDK, retaining original directory hierarchy/filenames
- Changed all class object instances to std::shared_ptrs to avoid memory leaks and ensure correct clean up of objects
- Researching suitable hashing algorithms for filename encryption and file integrity
    - Do we hash only the filename or hash the whole file?
        - Depends if we use the same hash for integrity/checksum purposes. May be faster to use a faster algorithm for integrity/checksums
    - CRC - only suitable for accidental data corruption, does not protect against malicious tampering
    - SHA-1 and md5 unsuitable for encryption purposes - only suitable for checksums
        - if using salts does this become more suitable?
    - SHA-2 with 256 bit hash should be sufficient for file integrity - can test vs 512 bit hash for speed
        - libraries:
            - PicoSHA2 - unknown implementation with STL, header only include
            - OpenSSL - known implementation, well tested
    - SHA-3 - ? SHA-2 attacks are becoming more practical - possible future proof with SHA-3 but potentially slower than SHA-2
        - todo: update code to implement SHA-3
    - Possible attack vector is compute large rainbow table of possible/known paths/filenames or files, and compare hashes 
        - therefore a random salt should be generated and hashed with the filename or binary file database
            - if we are using the same hash for file integrity we need to store the salt to later compare - can do so by storing hash in index as std::pair<hash, salt>
            - otherwise if not using the hash for integrity, since the hashing is only computed once, we do not need to store the salt. only store the salted hash in index
    - If we're not using for integrity, what's the benefit of hashing anything with a salt, not just generating random output - benefits of ensuring uniqueness?
    - S3 automatically generates MD5 hash of data uploaded and stores in an ETag - however this is different for files uploaded via multipart (https://stackoverflow.com/questions/12186993/what-is-the-algorithm-to-compute-the-amazon-s3-etag-for-a-file-larger-than-5gb/19896823#19896823)

Mon 6/7/2020
- Determined S3 does not retain original modtime when uploading - S3 last modtime is time first uploaded to S3
- Created stub abstract class Encryption.h (pure virtual) to handle encryption and hashing functions
    - Included OpenSSL library, linked library within CMAKE
    - hashPath() - simple hash of filename and random salt with SHA-256, stored in index and DB and uploaded to S3 with this hash instead of full path
        - random salt ensures that identical paths hash to different digests to avoid filename analysis with rainbow tables as above (how secure is this - need to add more random characters for more entropy?)
            - adding one random character from the pool of 62 adds 62 bits of entropy. for each filename in rainbow table, would have to add 62 different versions.
            - adding two random characters adds 62C2 = 1891 different bit of entropy = 1891 different versions for each potential path in rainbow table.
            - adding 128 random characters = ??????? - how long would it take to generate a rainbow table for one specific path? test this as part of paper
- Restore fileIndex and dirIndex from index.db on startup
- todo - possible feature, we can also hash files with modtime and save X amount of versions? use a multi-map to store multiple versions of the same file in index

Tue 7/7/2020
- Delete from S3 if file is removed on local filesystem
    - todo: cross reference cloud files vs local and add to upload queue if appropriate? some files may fail to delete/add we can check for this
- problem: if accidently deleted on local, it's removed from DB and from remote - so cannot restore this backup - it's a one way sync, do we need to keep on remote and keep in index
    - rclone this is sync vs copy - sync ensures local:remote is 1:1, copy doesn't delete on remote
- Multiple versions of the same file can now be stored/tracked.
    - Store fileIndex map with path as key, value being a vector of FileVersion struct objects. When a file is updated, we can push_back a FileVersion in the vector. This way the most up to date file can be pulled from the back of the vector and it gives us a chronological ordering. Considered a tuple instead of struct data type, but this is less flexible and syntax more verbose.
        - alternative was a multimap. multimap would allow duplicate keys with multiple values, however we are not guaranteed order when iterating through values without expensive ordered of the entire map. chronological ordering of vector approach above is preferred.
    - If file changes multiple times before file is encrypted/uploaded, the original file is not there to upload - so have to find someway to remove - check file with correct modtime exists before encrypting to avoid race conditions.
    - DB structure updated to accept multiple versions of files, fixed Watch::restoreFromDB() to correctly repopulate fileIndex map on start.
- Temporary exclusion for .swp temporary files added for testing until full exclusion system developed

Wed 8/7/2020
- Added flags in FileVersion struct to track if this file exists both locally and remotely. It may be deleted locally, but we still want to track to be able to restore from remote.
    - Ammended DB to include these flags
    - remoteExists flag is set and saved to DB on successful upload from remote
    - if file is changed or deleted before uploadQueue is executed, we cannot upload this version, so need to check modtime stamps before uploading and need to remove that version from vector
    - todo: before deleting from fileIndex we need to ensure that for ALL versions of a file (not only the most recent), they neither exist locally or remotely
- Interface problem - Watch needs to be able to interact with specific instance of Remote, and Remote needs to interact with specific instance of Watch - this creates a circular problem as we cannot pass pointers in constructors in this circular way due to the order of each class being instantiated. If we first create Remote we cannot pass a pointer to Watch as it is not yet instantiated, and vice versa.
    - Solved by passing shared pointers to the objects after all construction has completed.
    - For S3 class which is spawned by Remote, we passed a regular pointer to the Remote class instance with "this" key in S3 constructor, as S3 needs to call specific functions on Remote.

Thur 9/7/2020
- Changed S3 to inherit from Queue instead of spawning an instance 
    - Remote can now call queue methods directly i.e. Remote -> Queue, without having a middle man method to pass the function call Remote -> S3 -> Queue
- Research on design patterns and Singletons, and other techniques compared to current dependency injection
    - research on inheritance over currently implemented composition
    - some modules do need to be single instances for resource contention e.g. DB, Watch when multi-threading
- Ammended code to use Aws::Transfer::TransferManager to upload/download
    - Supports DownloadToDirectory() - download entire S3 bucket to directory
    - https://stackoverflow.com/questions/57520608/amazon-example-transfermanager-code-does-not-compile
    - https://github.com/aws/aws-sdk-cpp/blob/master/aws-cpp-sdk-transfer-tests/TransferTests.cpp
- Implememented download queue similar to queue system used for uploads

Fri 10/7/2020
- Fixed download locations 
    - all directories in the download path need to be created before attempting to download to that location, otherwise download states successful but nothing is saved on disk
    - todo: will need testing/further error checking when handling user input e.g. trailing slash vs no trailing slash

Sun 12/7/2020
- Research on Boost::ASIO - for handling local unix domain socket for interprocess communication between daemon and control/command program
    - built, tested and ammended example code for local unix domain sockets
- Split project into encloned (daemon/service) and enclone (CLI control application)

Mon 13/7/2020
- Integrated local socket test code in encloned and enclone, split into header/implementation
- Tested concurrent socket connections - works correctly
- Set permissions of IPC socket (/tmp/encloned) to owner only (+0700 rwx------) to restrict access to other users - only same user that is running daemon can control
- todo: 1. devise a messaging system/function caller
        2. cmd line arguments       
- Starting to research file encryption possibilities
    - see encryption.txt
- Looked into command line argument parsing options
    - boost::program_options vs hand written
        - boost allows easy parsing, extension for config files, clear syntax and error handling, easy capture of multiple files given as arguments and multiple flags
- Tested basic daemon function call from command sent through local socket - call addWatch() method in Watch.h with supplied path to CLI

Tue 14/7/2020
- Added basic CLI argument functionality with boost::program_options
- Included functionality to pipe a response to a command from daemon (encloned) back to client (enlcone) that made the request
    - methods return a response string, this is passed back to client which reads until the ";" delimiter, indicating end of reply
- CLI can now accept multiple arguments at once, e.g. multiple files to watch

Wed 15/7/2020
- Moved program options from main() into enclone class
- Changed daemon request to follow format of "command|args" with a "|" as delimiter - allows for better daemon side parsing
- Adding "--list" CLI option - enables showing available files both locally and on remote
    - todo: show all versions of a given file - "--versions" CLI cmd

Thur 16/7/2020
- Implement "--list remote" CLI argument
    - Made S3::callAPI() more generic to be able to make a listObjects call from outside, e.g. from CLI argument
    - Each API call is encapsulated with the AWS initAPI and shutdownAPI commands - can we instead have persistant s3_client and transferMgr? transferMgr did not appear to work last time this was attempted (possibly executor related)

Fri 17/7/2020
- fileHashIndex as a map of fileHash (key) and path (value) for easy lookup/resolution of hashes to paths
    - reduce computation time when looking up a fileHash - avoids looping through entire fileIndex and values multiple times to find a match
    - add hash to map when created, no additional information required to be stored in DB, we can restore from existing information
- Resolve "--list remote" pathHashes obtained from S3 into a list of local paths and mod time
- Error handling/exceptions added for listObjects call to callAPI - returns error to both CLI and daemon
- Error handling for uploadQueue - if upload fails, do not remove from queue until successful - upload will retry until successful
    - Improved handling of queue from S3 - simple loop through queue items, only dequeueing when upload is successful, otherwise printing full error message from S3 API
- Error handing for resolvePathHash() where hash/object exists on S3, but index does not have a corresponding file path
    - todo: Need a clean-up option to clear these items from S3
- When adding to uploadQueue, we also include modtime and check this modtime is the same before uploading file with a particular hash
    - Ensures we do not upload a different version of the file that requested with the wrong hash
    - The file version that no longer exists locally is removed from upload queue but still exists in index - this will need to be cleaned up later (where !remoteExists)
- todo: if uploadQueue fails e.g. invalid credentials, need to save this information to be able to restore it in event of program crash? can get this from fileIndex with !remoteExists, and that path with that modtime still exists
- todo: if files are deleted from S3 from outside, need to handle this - check for this - REMOTEEXISTS needs to be set false

Tue 21/7/2020
- Error handling/return message for empty S3 bucket on --list remote
- Error handling/return message for empty local index on --list local
- Fix remoteExists flag bug on successful upload - this was not being updated due to copy of vector being made, rather than use a pointer to access reference directly
- Implemented --restore CLI flag, currently downloads all files on S3 to /enclone/dl directory
    - preserves original modtime on filesystem when downloading files
        - modtime of directories is NOT currently preserved as this information is not saved
    - now CLI is fairly complete allows for easier testing of upload/download of encrypted files
- Further research on encryption algos - see encryption.txt
    - Built example OpenSSL AES CBC code with string values

Wed 22/7/2020
- Further research on file encryption - see encryption.txt
    - Authentication is required, but some types like AES-GCM cannot be used without loading entire files to memory - this is unfeasible for large files
        - Potentially this can be fixed by splitting files into chunks
        - Possible solution is XChaCha20-Poly1305 from Crypto++ or Libsodium
    - Another solution is use a backend like age to handle encryption/auth, although age has authentication concerns
- Built and tested various openSSL demos
    - openssl-evp-demo - file encryption in C
    - evp-encrypt.cxx - C++ implementation but encrypting strings not files
    - gcmcrypt - GCM file encryption in C
- todo:
    - read some up to date papers on AEAD to see the latest developments/recommendations, especially in regards to chunking/streaming large files (seems to be an difficult/unsolved problem, particularly authenticating the ordering of chunks - although NaCL streaming implementation seems to resolve this)

Thurs 23/7/2020
- Reading/collecting various cloud storage papers on data security/encryption

Fri 24/7/2020
- Further research/reading on file encryption
- Build and install Libsodium encryption library (NaCL)
- Tested NaCL xchacha20poly1305 streaming encrypt/decrypt in /test/NaCL
- Started to integrate NaCL streaming encryption methods in Encryption.h
- Key management
    - Created "--generate-key" option to enclone - generate and save random encryption key to file
        - keyfile has owner+rw permissions set BEFORE key is written to file to protect key
    - encloned loads encryption key from file, or exits with error is it does not exist
- todo: move both index.db and key file to user home directory? .enclone directory with restricted unix permissions

Sat 25/7/2020
- Encrypted file on upload to S3
    - File is encrypted to temporary location created on start (/tmp/enclone/) and deleted once upload is confirmed
- todo: 
    - hash check of files on download
    - write a test - upload files to S3, check they are encrypted, check they decrypt with hashes
        - can generate files with dd if=/dev/urandom bs=1M count=10 of=test.bin
    - backup index to S3 (replace, not version)

Mon 27/7/2020
- Decrypt file on download from S3
    - File is downloaded to temp location /tmp/enclone/ and deleted once decryption to dl directory confirmed
    - todo: move download/decrypt to separate threads? don't want to wait for decryption to finish before downloading next file - decrypt queue?
- Added --del-watch and --del-recursive CLI arguments - delete watches/files, including removing from remote backends

Tue 28/7/2020
- Various research on filename hash security - calculation of how long it would take to brute force filenames
- Added --target CLI argument to be used with --restore - specify the target directory to download files to
- Added support for --restore "path" and --restore "pathhash" to restore a particular version of a file by giving the hash or the latest version by providing path

Wed 29/7/2020
- Added error handling to downloadQueue and deleteQueue, and to make consistent with uploadQueue
    - downloadQueue will fail and print to CLI - need to manually retry
    - deleteQueue will keep retrying while program is open and remotes are called
    - todo: backup deleteQueue on exit - if we exit and deleteQueue has not been successfull, we will lose queue. can add a flag to db - "forDeletion"?
    - todo: retry upload - for files that have remoteExists == false on startup, re-add to upload queue to try again
- Added --clean-up CLI argument - sometimes we may have old files in S3 bucket that have no corresponding entry in fileIndex (e.g. if we remove index.db and build a new db). This option cleans up the S3 bucket.
- Added a fileHash() implementation in Encryption.h using NaCL that hashes a file in chunks of BUFFER_SIZE
    - each new FileVersion contains a hash of the file contents, so contents can be verified after download
- Integrity checking - after file download and decryption, we re-hash file and verify with saved file hash for further integrity check
    - is this extra authentication necessary? since encryption is authenticated, decryption will fail if file has changed in any way - poly1305 already provides integrity
    - however, using AE alone does not protect against a roll back - if the file on S3 is replaced with a previous authenticated file, it will successfully decrypt but provide out of date information
        - https://crypto.stackexchange.com/questions/42289/how-to-provide-integrity-even-though-using-authenticated-encryption
        
- todo: downloading multiple versions - no clobber - need to change filename to include modtime on older version and test this
- todo: save only X levels deep of file versions - config option
- todo: --restore path supports downloading all files in directory if directory provided (and optional recursive flag)
- todo: remove int and bool return codes? we should be using exceptions to return errors, except where we rely on bool returns
    - loadEncryptionKey() is one example
- todo: make queue into template, more generalised - to reuse for encryption queue

Tue 4/8/2020
- Replaced the sha256 hash of the path/filename + salt with a pure random 64 character string for increased security
    - the sha256 hash was not otherwise used or checked, now there is no relation between the original filename and the new filename stored remotely
    - why are we hashing filenames/paths - rather than encrypting with the same key? that way we can decrypt files to correct filenames even if index is lost but we still have the key
        - encryption would need to be to a readable, filename friendly ascii format - not random binary/bit info - would require some kind of format-preserving encryption or base64 encryption
        - we would also need to store with the nonce somehow - otherwise identical path names will encrypt to identical strings (for different versions of one file)
            - it is considered safe to store the nonce alongside the ciphertext (concat'd) - secret key is still unknown
        - some remotes are care insensitive, so possible base32 encoding (no upper case) may be required

Fri 14/8/2020
- Check validity of database after opening, and set permissions to ONLY owner read and write for local security